{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATP Tennis Data - Model Tuning For Graident Boosting\n",
    "\n",
    "\n",
    "In our previous notebooks, we removed some columns from our feature set and found that it had little effect on our model.\n",
    "\n",
    "In this notebook, we will use the same feature columns and tune our Gradient Boosting model to see if we can improve our performance\n",
    "\n",
    "### Before tuning\n",
    "\n",
    "```\n",
    "Model Score: 0.6903443619176233\n",
    "\n",
    "ROC/AUC Score: 0.6903161608528401\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        Loss       0.69      0.68      0.69      7381\n",
    "         Win       0.69      0.70      0.69      7429\n",
    "\n",
    "    accuracy                           0.69     14810\n",
    "   macro avg       0.69      0.69      0.69     14810\n",
    "weighted avg       0.69      0.69      0.69     14810\n",
    "```\n",
    "\n",
    "\n",
    "# Summary of Results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from util import jupyter_util as ju\n",
    "from util.model_util import ModelWrapper, REPORT_FILE, RSTATE, N_JOBS, MAX_ITER, LABEL_COL\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# date\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "DATE = datetime.now().strftime(DATE_FORMAT)\n",
    "\n",
    "DESCRIPTION = \"ohe-reduced_history_matchup\"\n",
    "\n",
    "\n",
    "# update this\n",
    "FEATURE_FILE = f'../datasets/atp_matches_1985-2019_features-ohe-history5-matchup5.csv'\n",
    "\n",
    "START_YEAR = 1998\n",
    "END_YEAR = 2018\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train, y_test = ju.get_data(FEATURE_FILE, LABEL_COL, START_YEAR, END_YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset actually has both historical data as well as matchup data. We will remove columns we are not using from this datast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before: data.shape (44429, 5299)\n",
      "After: data.shape (44429, 5044)\n",
      "\n",
      "Before: data.shape (14810, 5299)\n",
      "After: data.shape (14810, 5044)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def filter_features(data: pd.DataFrame):\n",
    "    \n",
    "    print(f'\\nBefore: data.shape {data.shape}')\n",
    "\n",
    "    new_features = data[[\"p1_rank\", \"p2_rank\", \"p1_seed\", \"p2_seed\", \"p1_history_games_won_percentage_diff\", \"p1_history_sets_won_percentage_diff\", \n",
    "                        \"p1_ht\", \"p2_ht\", \"p1_age\", \"p2_age\", \"p1_matchup_games_won_percentage\", \"p2_matchup_games_won_percentage\",\n",
    "                        \"p1_history_wins_diff\", \"tourney_level_label\", \"p1_matchup_sets_won_percentage\", \"p2_matchup_sets_won_percentage\",\n",
    "                        \"tourney_month\", \"round_label\"]]\n",
    "             \n",
    "    surface_cols = [col for col in data.columns if re.match(\"surface_\", col)]\n",
    "    new_features = pd.concat([new_features, data[surface_cols]], axis=1)\n",
    "\n",
    "    best_of_cols = [col for col in data.columns if re.match(\"best_of_\", col)]\n",
    "    new_features = pd.concat([new_features, data[best_of_cols]], axis=1)\n",
    "             \n",
    "    player_ioc_cols = [col for col in data.columns if re.match(r\"(p1|p2)_ioc_\", col)]\n",
    "    new_features = pd.concat([new_features, data[player_ioc_cols]], axis=1)\n",
    "\n",
    "    player_id_cols = [col for col in data.columns if re.match(r\"(p1|p2)_[\\d]+\", col)]\n",
    "    new_features = pd.concat([new_features, data[player_id_cols]], axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'After: data.shape {new_features.shape}')\n",
    "    return new_features\n",
    "\n",
    "X_train = filter_features(X_train_orig)\n",
    "X_test = filter_features(X_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed: ['draw_size', 'tourney_year', 'p1_hand_l', 'p1_hand_r', 'p1_hand_u', 'p2_hand_l', 'p2_hand_r', 'p2_hand_u', 'tourney_id_0301', 'tourney_id_0308', 'tourney_id_0311', 'tourney_id_0314', 'tourney_id_0315', 'tourney_id_0316', 'tourney_id_0319', 'tourney_id_0321', 'tourney_id_0322', 'tourney_id_0328', 'tourney_id_0329', 'tourney_id_0337', 'tourney_id_0341', 'tourney_id_0352', 'tourney_id_0360', 'tourney_id_0375', 'tourney_id_0402', 'tourney_id_0407', 'tourney_id_0410', 'tourney_id_0414', 'tourney_id_0421', 'tourney_id_0424', 'tourney_id_0425', 'tourney_id_0429', 'tourney_id_0439', 'tourney_id_0451', 'tourney_id_0495', 'tourney_id_0496', 'tourney_id_0499', 'tourney_id_0500', 'tourney_id_0506', 'tourney_id_0533', 'tourney_id_0568', 'tourney_id_0605', 'tourney_id_0717', 'tourney_id_0741', 'tourney_id_0773', 'tourney_id_0891', 'tourney_id_1536', 'tourney_id_1720', 'tourney_id_201', 'tourney_id_215', 'tourney_id_224', 'tourney_id_2276', 'tourney_id_237', 'tourney_id_240', 'tourney_id_301', 'tourney_id_302', 'tourney_id_303', 'tourney_id_304', 'tourney_id_305', 'tourney_id_306', 'tourney_id_308', 'tourney_id_309', 'tourney_id_310', 'tourney_id_311', 'tourney_id_312', 'tourney_id_313', 'tourney_id_314', 'tourney_id_315', 'tourney_id_316', 'tourney_id_317', 'tourney_id_318', 'tourney_id_319', 'tourney_id_321', 'tourney_id_322', 'tourney_id_323', 'tourney_id_325', 'tourney_id_326', 'tourney_id_327', 'tourney_id_328', 'tourney_id_329', 'tourney_id_330', 'tourney_id_332', 'tourney_id_3348', 'tourney_id_336', 'tourney_id_337', 'tourney_id_338', 'tourney_id_339', 'tourney_id_341', 'tourney_id_3465', 'tourney_id_348', 'tourney_id_352', 'tourney_id_354', 'tourney_id_357', 'tourney_id_359', 'tourney_id_360', 'tourney_id_366', 'tourney_id_367', 'tourney_id_375', 'tourney_id_379', 'tourney_id_401', 'tourney_id_402', 'tourney_id_403', 'tourney_id_404', 'tourney_id_405', 'tourney_id_406', 'tourney_id_407', 'tourney_id_408', 'tourney_id_409', 'tourney_id_410', 'tourney_id_411', 'tourney_id_412', 'tourney_id_414', 'tourney_id_415', 'tourney_id_416', 'tourney_id_417', 'tourney_id_418', 'tourney_id_419', 'tourney_id_420', 'tourney_id_421', 'tourney_id_422', 'tourney_id_423', 'tourney_id_424', 'tourney_id_425', 'tourney_id_426', 'tourney_id_427', 'tourney_id_428', 'tourney_id_429', 'tourney_id_430', 'tourney_id_433', 'tourney_id_434', 'tourney_id_435', 'tourney_id_436', 'tourney_id_437', 'tourney_id_438', 'tourney_id_439', 'tourney_id_440', 'tourney_id_441', 'tourney_id_450', 'tourney_id_451', 'tourney_id_456', 'tourney_id_457', 'tourney_id_463', 'tourney_id_468', 'tourney_id_473', 'tourney_id_475', 'tourney_id_481', 'tourney_id_494', 'tourney_id_495', 'tourney_id_496', 'tourney_id_497', 'tourney_id_498', 'tourney_id_499', 'tourney_id_500', 'tourney_id_5012', 'tourney_id_5014', 'tourney_id_505', 'tourney_id_5053', 'tourney_id_506', 'tourney_id_517', 'tourney_id_520', 'tourney_id_533', 'tourney_id_540', 'tourney_id_544', 'tourney_id_560', 'tourney_id_568', 'tourney_id_573', 'tourney_id_580', 'tourney_id_6003', 'tourney_id_604', 'tourney_id_605', 'tourney_id_610', 'tourney_id_6116', 'tourney_id_6120', 'tourney_id_615', 'tourney_id_620', 'tourney_id_6242', 'tourney_id_6710', 'tourney_id_6718', 'tourney_id_6932', 'tourney_id_6967', 'tourney_id_708', 'tourney_id_7161', 'tourney_id_7163', 'tourney_id_717', 'tourney_id_727', 'tourney_id_7290', 'tourney_id_73', 'tourney_id_741', 'tourney_id_7434', 'tourney_id_747', 'tourney_id_7480', 'tourney_id_7485', 'tourney_id_7581', 'tourney_id_7648', 'tourney_id_7650', 'tourney_id_7694', 'tourney_id_7696', 'tourney_id_773', 'tourney_id_80', 'tourney_id_807', 'tourney_id_88', 'tourney_id_890', 'tourney_id_891', 'tourney_id_92', 'tourney_id_96', 'tourney_id_m001', 'tourney_id_m004', 'tourney_id_m006', 'tourney_id_m007', 'tourney_id_m009', 'tourney_id_m010', 'tourney_id_m014', 'tourney_id_m015', 'tourney_id_m016', 'tourney_id_m020', 'tourney_id_m021', 'tourney_id_m024', 'tourney_id_m035', 'tourney_id_m052', 'tourney_id_o16', 'p1_history_games_lost', 'p1_history_games_won', 'p1_history_games_won_percentage', 'p1_history_losses', 'p1_history_sets_lost', 'p1_history_sets_won', 'p1_history_sets_won_percentage', 'p1_history_win_percentage', 'p1_history_wins', 'p1_matchup_games', 'p1_matchup_losses', 'p1_matchup_sets', 'p1_matchup_win_percentage', 'p1_matchup_wins', 'p2_history_games_lost', 'p2_history_games_won', 'p2_history_games_won_percentage', 'p2_history_losses', 'p2_history_sets_lost', 'p2_history_sets_won', 'p2_history_sets_won_percentage', 'p2_history_win_percentage', 'p2_history_wins', 'p2_matchup_games', 'p2_matchup_losses', 'p2_matchup_sets', 'p2_matchup_win_percentage', 'p2_matchup_wins', 'p1_history_win_percentage_diff', 'p1_history_losses_diff', 'p1_matchup_wins_diff', 'p1_matchup_games_diff', 'p1_matchup_sets_diff', 'p1_history_matches', 'p2_history_matches']\n"
     ]
    }
   ],
   "source": [
    "print(f'Columns removed: {[col for col in X_test_orig.columns if col not in X_test.columns]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our Best Estimater to give us a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.read_csv(REPORT_FILE)\n",
    "current_report = report[(report.model_name == 'GradientBoostingClassifier') &\n",
    "                                  (report.description == \"ohe-reduced_history_matchup\")]\n",
    "mw = ModelWrapper.get_model_wrapper_from_report(current_report)\n",
    "mw.model.n_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree - Grid Search 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 900 candidates, totalling 4500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt = GradientBoostingClassifier(random_state=RSTATE, verbose=1, n_iter_no_change = 4)\n",
    "parameters = {'n_estimators': [100, 200, 300],\n",
    "              'min_samples_split': [2, 4, 8, 16],  \n",
    "              'min_samples_leaf': [1, 2, 4, 8, 16], \n",
    "              'max_depth': [1, 3, 6, 12, 24], \n",
    "              'max_features': [None, 'sqrt', 'log2'],\n",
    "            }\n",
    "gscv = GridSearchCV(dt, parameters, cv=5, scoring='accuracy', verbose = 1, refit = True, n_jobs = N_JOBS)\n",
    "gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(gscv.cv_results_)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[gscv.best_index_].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_list = results_df[\"param_max_features\"].unique()\n",
    "replace_dict = { max_features_list[idx]: idx for idx in np.arange(len(max_features_list)) }\n",
    "replace_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"param_max_features\"] = results_df[\"param_max_features\"].replace(replace_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.iloc[gscv.best_index_].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[\"param_n_estimators\", \"param_max_depth\", \"param_max_features\", \"param_min_samples_leaf\", \"param_min_samples_split\", \"mean_test_score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Results\n",
    "\n",
    "* XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(5, 1, figsize=(20, 25))\n",
    "\n",
    "ax = results_df[\"param_max_depth\"].plot(ax=a[0], legend=True)\n",
    "results_df[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv.best_index_, 0, results_df[\"param_max_depth\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Max Depth vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "ax = results_df[\"param_max_features\"].plot(ax=a[1], legend=True)\n",
    "results_df[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv.best_index_, 0, results_df[\"param_max_features\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Max Features vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n",
    "ax = results_df[\"param_min_samples_leaf\"].plot(ax=a[2], legend=True)\n",
    "results_df[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv.best_index_, 0, results_df[\"param_min_samples_leaf\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Min Sample Leaf vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "ax = results_df[\"param_min_samples_split\"].plot(ax=a[3], legend=True)\n",
    "results_df[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv.best_index_, 0, results_df[\"param_min_samples_split\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Min Sample Split vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n",
    "ax = results_df[\"param_n_estimators\"].plot(ax=a[4], legend=True)\n",
    "results_df[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv.best_index_, 0, results_df[\"param_n_estimators\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"N_estimators vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will do another Grid Search to fine tune some of our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# estimator2 = GradientBoostingClassifier(random_state=RSTATE, verbose=1, n_iter_no_change = 4)\n",
    "# parameters = { 'min_samples_split': [26, 28, 30, 32, 34, 36, 38, 40, 50],  'min_samples_leaf': [3, 4, 5, 6, 7]}\n",
    "# gscv2 = GridSearchCV(estimator2, parameters, cv=5, scoring='accuracy', verbose = 1, refit = True, n_jobs = N_JOBS, return_train_score = True)\n",
    "# gscv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.DataFrame(gscv2.cv_results_)\n",
    "results_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv2.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like our results are still the same as before with min sample leaf at 4 and min smaple split at 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "ax = results_df2[\"param_min_samples_leaf\"].plot(ax=a[0], legend=True)\n",
    "results_df2[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv2.best_index_, 0, results_df2[\"param_min_samples_leaf\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Min Sample Leaf vs Test Score\")\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "ax = results_df2[\"param_min_samples_split\"].plot(ax=a[1], legend=True)\n",
    "results_df2[\"mean_test_score\"].plot(ax=ax.twinx(), legend=True, color='r')\n",
    "ax.axvline(gscv2.best_index_, 0, results_df2[\"param_min_samples_split\"].max(), c='orange', linewidth=5)\n",
    "ax.set_title(\"Min Sample Split vs Test Score\")\n",
    "ax.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare our model accuracy against our test datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model from our Original Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw = ModelWrapper(gscv.best_estimator_,\n",
    "                description = DESCRIPTION, \n",
    "                 data_file = FEATURE_FILE,\n",
    "                  start_year = START_YEAR,\n",
    "                  end_year = END_YEAR,\n",
    "                   X_train = X_train,\n",
    "                   y_train = y_train,\n",
    "                   X_test = X_test,\n",
    "                   y_test = y_test)\n",
    "y_predict_dt = mw.predict()\n",
    "mw.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model from our 2nd Grid search\n",
    "\n",
    "Looks like there is a slight improvement in accuracy compared to our first grid search\n",
    "\n",
    "Although recall for Losses dropped by a percent, our precision actually increased by 1% - meaning that we are slightly worse at identifying losses but when we do, it tends to be more accurate\n",
    "\n",
    "Win precision also decreased by 1% but our recall for Wins increased by 1% - meaning we are better at identifying wins in our predictions, however, out of these wins there are more false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw2 = ModelWrapper(gscv2.best_estimator_,\n",
    "                description = DESCRIPTION, \n",
    "                 data_file = FEATURE_FILE,\n",
    "                  start_year = START_YEAR,\n",
    "                  end_year = END_YEAR,\n",
    "                   X_train = X_train,\n",
    "                   y_train = y_train,\n",
    "                   X_test = X_test,\n",
    "                   y_test = y_test)\n",
    "y_predict_dt2 = mw2.predict()\n",
    "mw2.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving off our model in case we need it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw2.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
